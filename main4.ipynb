{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "greeks = pd.read_csv('./data/greeks.csv')\n",
    "test = pd.read_csv('./data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index\n",
    "\n",
    "train = train.set_index('Id')\n",
    "test = test.set_index('Id')\n",
    "greeks = greeks.set_index('Id')\n",
    "\n",
    "# Separate target and features\n",
    "X = train.drop('Class', axis = 1)\n",
    "y = train['Class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Data Organization / Cleaning </center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features list\n",
    "violin_features = list(X.columns)\n",
    "categorial_features = X.columns[X.dtypes != float].values.tolist()\n",
    "\n",
    "# Remove non-float features\n",
    "violin_features.remove(*categorial_features)\n",
    "\n",
    "print(categorial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding for 'EJ' categorial feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorial object 'EJ' feature \n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(X['EJ'].values.reshape(-1, 1))\n",
    "onehot_cols = ['EJ' + '_' + x for x in enc.categories_[0]]\n",
    "\n",
    "X[onehot_cols] = enc.transform(X['EJ'].values.reshape(-1, 1)).toarray()\n",
    "X = X.drop('EJ', axis = 1)\n",
    "\n",
    "#test[onehot_cols] = enc.transform(test['EJ'].values.reshape(-1, 1)).toarray()\n",
    "#test = test.drop('EJ', axis = 1)\n",
    "\n",
    "#violin_features.extend(onehot_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check and fill missing values using KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which features are missing and how many missing data points are there\n",
    "\n",
    "nas = X.isnull().sum()[X.isnull().sum() > 0]\n",
    "nas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the features which contain missing values are not normally-distributed, we will use iterative imputer. \n",
    "\n",
    "imp = KNNImputer() # weight by distance but use many neighbors\n",
    "X2 = pd.DataFrame(imp.fit_transform(X), index = X.index, columns = X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot violin plots of non-categorial features\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 7, ncols = 1, figsize = (15, 20))\n",
    "\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for row in range(7):\n",
    "    features = violin_features[row*8:(row+1)*8]\n",
    "    #df_plot = pd.concat([X2[features], y], axis = 1)\n",
    "    df_plot = pd.concat([X2[features], y], axis = 1)\n",
    "    df_plot = df_plot / df_plot.max(axis = 0) # Normalize them for plotting purposes\n",
    "    df_plot = df_plot.melt(id_vars = ['Class'], value_vars = features)\n",
    "    sns.violinplot(data = df_plot, x = 'variable', y = 'value', hue = 'Class', split = True,palette = 'viridis', ax = axs[row])\n",
    "    sns.stripplot(data = df_plot, x = 'variable', y = 'value', hue = 'Class', dodge = True,palette = 'viridis', alpha = 0.2, ax = axs[row])\n",
    "    axs[row].get_legend().remove()\n",
    "sns.despine()\n",
    "#plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semi-insights:\n",
    "*Highly concentrated AY, BC, BR, BZ, DU, EH, EU, FR  <br>\n",
    "* Large AM -> Highly likely Green <br>\n",
    "* Large CR -> Slightly likely blue <br>\n",
    "* Large DH -> Slightly likely blue <br>\n",
    "* Large DA -> Slightly likely blue <br>\n",
    "* Large EE -> Slightly likely blue <br>\n",
    "* Large FI -> Slightly likely blue <br>\n",
    "<br>\n",
    "\n",
    "Before studying the correlations, let us complete the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr = X2.corr().abs()\n",
    "corr_pairs = df_corr.unstack().dropna().sort_values(ascending=False)\n",
    "selected_corr_pairs = corr_pairs[(corr_pairs < 1) & (corr_pairs > 0.75)]\n",
    "selected_corr_pairs = selected_corr_pairs.iloc[::2] # drop the identical \n",
    "selected_corr_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spaces?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot calibration plots of highly correlated categorial features\n",
    "\n",
    "fig, axs = plt.subplots(nrows = 4, ncols = 3, figsize = (12, 8))\n",
    "\n",
    "axs = np.ravel(axs)\n",
    "\n",
    "for i, pair in enumerate(selected_corr_pairs.index):\n",
    "    f1, f2 = pair\n",
    "    sns.scatterplot(data = pd.concat([X2[[f1, f2]], y], axis = 1), x = f1, y = f2, hue = 'Class', ax = axs[i], palette='viridis')\n",
    "    #sns.regplot(data = train[[f1, f2, 'Class']], x = f1, y = f2, ax = axs[i])\n",
    "\n",
    "    axs[i].get_legend().remove()\n",
    "    axs[i].set_xticks([])\n",
    "    axs[i].set_yticks([])\n",
    "sns.despine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorial = train[['GL', 'EJ', 'Class']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 2, nrows = 1, figsize = (10, 4))\n",
    "sns.histplot(data = train[train['Class'] == 0], x = 'GL', hue = 'EJ', ax = ax[0], hue_order = ['A', 'B'], bins = 20)\n",
    "sns.histplot(data = train[train['Class'] == 1], x = 'GL', hue = 'EJ', ax = ax[1], hue_order = ['A', 'B'], bins = 20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Value count with GL over 18\\n',train.loc[train['GL'] >= 18, \"EJ\"].value_counts())\n",
    "print('Value count with GL under 18\\n',train.loc[train['GL'] < 18, \"EJ\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The single value with B ('+train.loc[(train['GL'] > 18) & (train['EJ'] == 'B')].index[0]+') is not the same as the missing value in GL ('+X['GL'][X['GL'].isnull()].index[0]+')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, smaller GL will be always in Class B, and larger A will always be in class A. \n",
    "<br>\n",
    "Let us map the correlated features in a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_corr_pairs = corr_pairs[(corr_pairs < 1) & (corr_pairs > 0.6)]\n",
    "selected_corr_pairs = selected_corr_pairs.iloc[::2] # drop the identical \n",
    "selected_corr_pairs\n",
    "G = nx.DiGraph(selected_corr_pairs.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_points_on_circle(nodes, radius=1):\n",
    "    points = {}\n",
    "    num_points = len(nodes)\n",
    "    for i in range(num_points):\n",
    "        angle = 2 * np.pi * i / num_points\n",
    "        x = radius * np.cos(angle)\n",
    "        y = radius * np.sin(angle)\n",
    "        points[nodes[i]] = (x, y)\n",
    "    return points\n",
    "\n",
    "# Generate 15 points on the circle with radius 1\n",
    "points_on_circle = generate_points_on_circle(list(list(G.nodes())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"font_size\": 16,\n",
    "    \"node_size\": 700,\n",
    "    \"node_color\": \"white\",\n",
    "    \"edgecolors\": \"black\",\n",
    "    \"linewidths\": 0.5,\n",
    "    \"width\": 0.5,\n",
    "}\n",
    "nx.draw_networkx(G, points_on_circle, **params)\n",
    "ax = plt.gca()\n",
    "ax.margins(0.2)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Dimensionality Reduction </center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try PCA.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "X2_reduced = pd.DataFrame(pca.fit_transform(X2 / X2.max()), index = X2.index) # PCA on scaled data\n",
    "X2_reduced = X2_reduced / X2_reduced.max()\n",
    "print('The 3 PCA components explain {:.2f}%, {:.2f}% ,and {:.2f}%, respectievly.'.format(*100*pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols = 2, nrows = 1, figsize = (8, 3.5))\n",
    "\n",
    "sns.histplot(data = pd.concat([X2_reduced, y], axis = 1), x = 0, hue = 'Class', ax = axs[0])\n",
    "sns.scatterplot(data = pd.concat([X2_reduced, y], axis = 1), x = 0, y = 1, hue = 'Class', ax = axs[1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = fig.add_subplot(projection=\"3d\", elev=30, azim=50)\n",
    "ax.scatter(\n",
    "    X2_reduced[0],\n",
    "    X2_reduced[1],\n",
    "    X2_reduced[2],\n",
    "    c=y,\n",
    "    cmap='bwr',\n",
    "    edgecolor=\"k\",\n",
    "    s=40,\n",
    ")\n",
    "ax.set_xlabel('0')\n",
    "ax.set_ylabel('1')\n",
    "ax.set_zlabel('2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The first component can clearly cluster the data to two clusters. \n",
    "* The distribution is not related to the class itslef.\n",
    "* The second component does not help to further cluster the data. The third is even less useful.<br>\n",
    "\n",
    "These observations are in line with the relatively high explained variance ratio of the first component compared to the second one. <br>\n",
    "Let us examine the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_importance = pd.DataFrame(pca.components_[0,:], index = X2.columns, columns = ['Importance']).abs().sort_values(ascending=False, by = \"Importance\").reset_index()\n",
    "pca_importance = pca_importance.rename({'index':'Features'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_importance['Importance cumsum'] = pca_importance['Importance'].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.barplot(data = pca_importance[0:10], y = 'Importance', x = 'Features')\n",
    "ax2 = ax.twinx()\n",
    "sns.lineplot(data = pca_importance[0:10], y = 'Importance cumsum', x = 'Features', ax = ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Baseline Models </center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "from sklearn.metrics import f1_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_logloss(y, p):\n",
    "    '''\n",
    "    Calculation of balance logloss according to the competition metric.\n",
    "    Input:\n",
    "        y   -   Real target\n",
    "        p   -   Predicted probabilities\n",
    "    Output:\n",
    "        L   -   Balanced logloss\n",
    "    '''\n",
    "    # Clip the propabilities to avoid function extremes\n",
    "    p = np.clip(p, 1e-15, 1-1e-15) \n",
    "\n",
    "    # Count targets\n",
    "    N_0 = y[y == 0].shape[0] \n",
    "    N_1 = y[y == 1].shape[0]\n",
    "    \n",
    "    # Arrays of ones for samples which belong to each target 0 \n",
    "    y0 = np.zeros_like(y) \n",
    "    y0[y == 0] = 1\n",
    "    y1 = np.zeros_like(y) \n",
    "    y1[y == 1] = 1\n",
    "\n",
    "    # Probability array for each target\n",
    "    p0 = p[:,0] \n",
    "    p1 = p[:,1] \n",
    "\n",
    "    # logloss of target, normalized to the number of samples for each target\n",
    "    logloss_0 = -np.sum(y0*np.log(p0))/(N_0+1e-15) \n",
    "    logloss_1 = -np.sum(y1*np.log(p1))/(N_1+1e-15) \n",
    "\n",
    "    L = (logloss_0 + logloss_1)/2\n",
    "\n",
    "    return L\n",
    "    \n",
    "# test\n",
    "# y_tmp = np.array([0, 0, 0, 0, 0])\n",
    "# p_tmp = np.array([[0.91, 0.09], # 0\n",
    "#                  [0.89, 0.11], # 0\n",
    "#                  [0.07, 0.93], # 1\n",
    "#                  [0.02, 0.98], # 1\n",
    "#                  [0.84, 0.16]]) # 0\n",
    "# balanced_logloss(y_tmp, p_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_train, X_test, y_train): # \n",
    "    '''\n",
    "    A function which imputes (knn), scales (standard scaler), and calculate class weights.\n",
    "    Input: \n",
    "        X_train   -   Training X dataset (df).\n",
    "        X_test    -   Testing X dataset (df).\n",
    "        y_train   -   Training target (df) for weighting purposes. \n",
    "    Output:\n",
    "        X_train   -   Scaled and imputed training X dataset (df).\n",
    "        X_test    -   Scaled and imputed testing X dataset (df).\n",
    "        sw        -   Samples weight based on the training target. \n",
    "    '''\n",
    "    # Add missing values using KNN Imputer\n",
    "    imp = KNNImputer(weights = 'distance', n_neighbors=20).fit(X_train)\n",
    "\n",
    "    X_train = pd.DataFrame(imp.transform(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(imp.transform(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    # Scale data using Standard Scaler\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.transform(X_train), index = X_train.index, columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), index = X_test.index, columns = X_test.columns)\n",
    "\n",
    "    # Class and sample weights\n",
    "    cw = compute_class_weight(class_weight = \"balanced\", classes = np.unique(y_train), y = y_train)\n",
    "    cw = dict(zip(np.unique(y_train), cw))\n",
    "    sw = compute_sample_weight(cw, y_train)\n",
    "\n",
    "    return X_train, X_test, imp, scaler, sw, cw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_score_store(i, X_train, y_train, X_test, y_test, models, models_names):\n",
    "    '''\n",
    "    A function which predicts, scores predictions, and calculate scores for the different models.\n",
    "    The function treats a single split.\n",
    "    Inputs:\n",
    "        i              -   Split number (int)\n",
    "        X_train        -   Training X dataset (df).\n",
    "        X_test         -   Testing X dataset (df).\n",
    "        y_train        -   Training target (df). \n",
    "        y_test         -   Testing target (df). \n",
    "        models         -   Trained models to be evaluated (string)\n",
    "        models_names   -   Trained models to be evaluated\n",
    "    Outputs:\n",
    "        score          -   Scores \n",
    "        y_hats         -   Predictions\n",
    "        ps             -   Predicted probabilities\n",
    "    '''\n",
    "    score, y_hats, ps = {}, {}, {}\n",
    "\n",
    "    # Iterate over models, store their predictions, probabilities and scores\n",
    "    for model, model_name in zip(models, models_names):\n",
    "        model_train_name = (model_name, 'train', str(i)) \n",
    "        model_test_name = (model_name, 'test', str(i))\n",
    "\n",
    "        # Store predicted probabilities\n",
    "        ps[model_train_name] = model.predict_proba(X_train) \n",
    "        ps[model_test_name] = model.predict_proba(X_test)\n",
    "\n",
    "        # Store predicted targets\n",
    "        y_hats[model_train_name] = model.predict(X_train) \n",
    "        y_hats[model_test_name] =  model.predict(X_test)\n",
    "\n",
    "        # Calculate scores\n",
    "        score[(model_name,str(i))] = [balanced_logloss(y_test, ps[model_test_name]),  \n",
    "                                      balanced_logloss(y_train, ps[model_train_name]),\n",
    "                                      f1_score(y_test, y_hats[model_test_name]), \n",
    "                                      f1_score(y_train, y_hats[model_train_name]),\n",
    "                                      accuracy_score(y_test, y_hats[model_test_name]), \n",
    "                                      accuracy_score(y_train, y_hats[model_train_name])] \n",
    "\n",
    "    \n",
    "    return score, y_hats, ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train(X_in, y_in, random_state):\n",
    "    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "\n",
    "    scores = {}\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(skf.split(X_in, y_in)):  \n",
    "        # Split\n",
    "        X_train, X_test = X_in.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y_in.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        X_train, X_test, imp, scaler, sw, cw = preprocess(X_train, X_test, y_train)\n",
    "\n",
    "        # RF\n",
    "        rf = RandomForestClassifier(criterion = 'log_loss')\n",
    "        rf.fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "        # GB\n",
    "        gb = GradientBoostingClassifier(loss = 'log_loss')\n",
    "        gb.fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "        # xgboost\n",
    "        xgb_clf = xgb.XGBClassifier(n_jobs = 28)\n",
    "        xgb_clf.fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "        # Predict, score and store\n",
    "        score_split, y_hats, ps = pred_score_store(i, X_train, y_train, X_test, y_test, [rf, gb, xgb_clf], ['rf', 'gb', 'xgb'])\n",
    "        scores = {**scores, **score_split}\n",
    "\n",
    "    scores_cols= ['balanced_logloss','balanced_logloss_train',\n",
    "                  'f1_score','f1_score_train',\n",
    "                  'accuracy_score','accuracy_score_train']\n",
    "    df_scores = pd.DataFrame(scores, index = scores_cols).T.sort_index()\n",
    "    \n",
    "    return(df_scores, y_hats, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for split in [42]:#[0, 1, 2, 7, 13, 25, 42, 67, 73]:\n",
    "    score, y_hats, ps = split_train(X, y, split)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.concat(scores).reset_index().groupby('level_0').describe().loc[:,(slice(None),['count','mean','std'])]\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center> Models Optimization using Optuna  </center></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "#from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_balanced_logloss(clf, X, y):\n",
    "    '''\n",
    "    Calculation of balance logloss according to the competition metric.\n",
    "    Input:\n",
    "        y   -   Real target\n",
    "        p   -   Predicted probabilities\n",
    "    Output:\n",
    "        L   -   Balanced logloss\n",
    "    '''\n",
    "    # Calculate p\n",
    "    p = clf.predict_proba(X)\n",
    "\n",
    "    # Clip the propabilities to avoid function extremes\n",
    "    p = np.clip(p, 1e-15, 1-1e-15) \n",
    "\n",
    "    # Count targets\n",
    "    N_0 = y[y == 0].shape[0] \n",
    "    N_1 = y[y == 1].shape[0]\n",
    "    \n",
    "    # Arrays of ones for samples which belong to each target 0 \n",
    "    y0 = np.zeros_like(y) \n",
    "    y0[y == 0] = 1\n",
    "    y1 = np.zeros_like(y) \n",
    "    y1[y == 1] = 1\n",
    "\n",
    "    # Probability array for each target\n",
    "    p0 = p[:,0] \n",
    "    p1 = p[:,1] \n",
    "\n",
    "    # logloss of target, normalized to the number of samples for each target\n",
    "    logloss_0 = -np.sum(y0*np.log(p0))/(N_0+1e-15) \n",
    "    logloss_1 = -np.sum(y1*np.log(p1))/(N_1+1e-15) \n",
    "\n",
    "    L = (logloss_0 + logloss_1)/2\n",
    "\n",
    "    return L\n",
    "    \n",
    "# test\n",
    "# y_tmp = np.array([0, 0, 0, 0, 0])\n",
    "# p_tmp = np.array([[0.91, 0.09], # 0\n",
    "#                  [0.89, 0.11], # 0\n",
    "#                  [0.07, 0.93], # 1\n",
    "#                  [0.02, 0.98], # 1\n",
    "#                  [0.84, 0.16]]) # 0\n",
    "# balanced_logloss(y_tmp, p_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_optimize_train(X_in, y_in, n_trials, random_state):\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5, random_state=random_state, shuffle=True)\n",
    "\n",
    "    rf_studies = {}\n",
    "    rf_bests = {}\n",
    "\n",
    "    xgb_studies = {}\n",
    "    xgb_bests = {}\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(X_in, y_in)):\n",
    "        X_train, X_val = X_in.iloc[train_index, :], X_in.iloc[val_index, :]\n",
    "        y_train, y_val = y_in.iloc[train_index], y_in.iloc[val_index]\n",
    "\n",
    "        X_train, X_val, imp, scaler, sw, cw = preprocess(X_train, X_val, y_train)\n",
    "        cw = cw[0] / cw[1]\n",
    "\n",
    "        # Optimize RF\n",
    "        def objective_rf(trial):\n",
    "            rf_params = {\n",
    "                \"criterion\": \"log_loss\",\n",
    "                \"random_state\":random_state,\n",
    "                \"n_estimators\":trial.suggest_int(\"n_estimators\", 70, 130), \n",
    "                 \"max_depth\": trial.suggest_int(\"max_depth\", 1, 600),\n",
    "                 \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 150),\n",
    "                 \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 2, 60),\n",
    "            }\n",
    "\n",
    "            rf_clf = RandomForestClassifier(**rf_params)\n",
    "            \n",
    "            rf_clf.fit(X_train, y_train, sample_weight = sw)\n",
    "\n",
    "            p_val = rf_clf.predict_proba(X_val)\n",
    "            \n",
    "            scores = balanced_logloss(y_val, p_val)\n",
    "\n",
    "            return scores\n",
    "\n",
    "        rf_study = optuna.create_study(direction=\"maximize\")\n",
    "        rf_study.optimize(objective_rf, n_trials=n_trials, n_jobs = -1)\n",
    "\n",
    "        rf_best = RandomForestClassifier(**rf_study.best_params).fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "        train_loss = balanced_logloss(y_train, rf_best.predict_proba(X_train))\n",
    "\n",
    "        rf_studies[(i,random_state)] = {**{\"train_score\": train_loss}, \n",
    "                         **{\"val_score\": rf_study.best_value}, \n",
    "                         **rf_study.best_params}\n",
    "        rf_bests[(i,random_state)] = rf_best\n",
    "\n",
    "\n",
    "        \n",
    "        # Optimize RF\n",
    "        def objective_xgb(trial):\n",
    "            xgb_params = {\n",
    "                 \"eval_metric\": \"logloss\",\n",
    "                 \"n_jobs\": -1,\n",
    "                 \"verbosity\":0,\n",
    "                 \"random_state\":random_state,\n",
    "                 \"verbose_eval\":False,\n",
    "                 \"objective\": \"binary:logistic\",\n",
    "                 \"booster\":\"gbtree\",\n",
    "                 #\"n_estimators\": 100,\n",
    "                 \"early_stopping_rounds\":20,\n",
    "                 \"scale_pos_weight\":cw,\n",
    "                 \"max_depth\":  trial.suggest_int(\"max_depth\", 1, 9), # trial.suggest_categorical('random_state', [1, 2, 3, 4, 5, 6, 7, 8, 9, None])\n",
    "                 \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log = True),\n",
    "                 \"eta\": trial.suggest_float(\"eta\", 1e-5, 1.0, log = True),\n",
    "                 \"lambda\": trial.suggest_float(\"lambda\", 1e-5, 1.0, log = True),\n",
    "                 \"gamma\": trial.suggest_float(\"gamma\", 1e-5, 1.0, log = True),\n",
    "                 \"alpha\": trial.suggest_float(\"alpha\", 1e-5, 1.0, log = True),\n",
    "                 \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "                 \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n",
    "            }\n",
    "\n",
    "            xgb_clf = xgb.XGBClassifier(**xgb_params)\n",
    "            \n",
    "            xgb_clf.fit(X_train, \n",
    "                        y_train, \n",
    "                        eval_set = [(X_val, y_val)], \n",
    "                        #random_state = random_state,\n",
    "                        verbose=False,\n",
    "                        sample_weight=sw)\n",
    "\n",
    "            p_val = xgb_clf.predict_proba(X_val)\n",
    "\n",
    "            scores = balanced_logloss(y_val, p_val)\n",
    "\n",
    "            return scores\n",
    "\n",
    "        xgb_study = optuna.create_study(direction=\"maximize\")\n",
    "        xgb_study.optimize(objective_xgb, n_trials=n_trials, n_jobs = -1)\n",
    "\n",
    "        xgb_best = xgb.XGBClassifier(**xgb_study.best_params).fit(X_train, y_train, sample_weight=sw)\n",
    "\n",
    "        train_loss = balanced_logloss(y_train, xgb_best.predict_proba(X_train))\n",
    "\n",
    "        xgb_studies[(i,random_state)] = {**{\"train_score\": train_loss}, \n",
    "                         **{\"val_score\": xgb_study.best_value}, \n",
    "                         **xgb_study.best_params}\n",
    "                         \n",
    "        xgb_bests[(i,random_state)] = xgb_best\n",
    "        \n",
    "    return(rf_studies, rf_bests, xgb_studies, xgb_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_studies_all = []\n",
    "rf_bests_all = []\n",
    "xgb_studies_all = []\n",
    "xgb_bests_all = []\n",
    "\n",
    "for split in [0, 1, 2, 7]:#, 13, 25, 42, 67, 73]:\n",
    "    rf_studies, rf_best, xgb_studies, xgb_best =  split_optimize_train(X, y, 5000, split)\n",
    "    xgb_studies = pd.DataFrame(xgb_studies).T\n",
    "    rf_studies = pd.DataFrame(rf_studies).T\n",
    "    \n",
    "    rf_studies_all.append(rf_studies)\n",
    "    rf_bests_all.append(rf_best)\n",
    "\n",
    "    xgb_studies_all.append(xgb_studies)\n",
    "    xgb_bests_all.append(xgb_best)\n",
    "    \n",
    "    xgb_studies.to_csv('xgb'+str(split)+'.csv')\n",
    "    rf_studies.to_csv('rf'+str(split)+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(xgb_studies_all).to_csv('xgb_studies.csv')\n",
    "pd.concat(rf_studies_all).to_csv('xgb_studies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
